"""
API Ä°ÅLEYÄ°CÄ°SÄ°
TÃ¼m AI provider'larla iletiÅŸimi burada yÃ¶netiyoruz
Gemini API tam entegre edildi!
Web Search (Tavily) eklendi! ğŸ”
"""

from config.settings import Settings
import requests

class MultiProviderAPIHandler:
    """Ã‡oklu AI provider'Ä± destekler"""
    
    def __init__(self, provider=None, model=None):
        self.provider = provider or Settings.DEFAULT_PROVIDER
        
        # MODEL SEÃ‡Ä°MÄ° DÃœZELTMESÄ°
        # Frontend bazen index gÃ¶nderiyor (0, 1, 2), bazen string gÃ¶nderiyor
        if model is not None:
            # EÄŸer sayÄ± (index) gÃ¶nderilmiÅŸse
            if isinstance(model, int) or (isinstance(model, str) and model.isdigit()):
                try:
                    model_index = int(model)
                    available = Settings.AVAILABLE_MODELS.get(self.provider, [])
                    if 0 <= model_index < len(available):
                        model = available[model_index]
                    else:
                        model = None
                except:
                    model = None
        
        # Model hala None ise varsayÄ±lanÄ± kullan
        if not model:
            model = Settings.DEFAULT_MODELS.get(self.provider)
        
        self.model = model
        self.client = self._init_client()
    
    def _init_client(self):
        """Provider'a gÃ¶re client oluÅŸtur"""

        if self.provider == "google":
            import google.generativeai as genai
            genai.configure(api_key=Settings.GOOGLE_API_KEY)
            return genai

        elif self.provider == "cerebras":
            from openai import OpenAI
            return OpenAI(
                api_key=Settings.CEREBRAS_API_KEY,
                base_url="https://api.cerebras.ai/v1"
            )

        else:
            raise ValueError(f"Desteklenmeyen provider: {self.provider}")

    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ” WEB SEARCH - TAVILY API
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @staticmethod
    def web_search(query):
        """
        Tavily API kullanarak web aramasÄ± yapar
        
        Args:
            query (str): Arama sorgusu
            
        Returns:
            dict: Arama sonuÃ§larÄ± veya hata mesajÄ±
        """
        try:
            # API key kontrolÃ¼
            if not Settings.TAVILY_API_KEY:
                return {
                    'success': False,
                    'error': 'Tavily API key tanÄ±mlanmamÄ±ÅŸ. .env dosyasÄ±na TAVILY_API_KEY ekleyin.'
                }
            
            # Tavily API endpoint
            url = "https://api.tavily.com/search"
            
            # Request payload
            payload = {
                "api_key": Settings.TAVILY_API_KEY,
                "query": query,
                "search_depth": "basic",  # "basic" veya "advanced"
                "max_results": Settings.SEARCH_MAX_RESULTS,
                "include_answer": True,  # AI Ã¶zeti dahil et
                "include_raw_content": False,  # Ham iÃ§erik gereksiz
                "include_images": False
            }
            
            # API'ye istek gÃ¶nder
            response = requests.post(url, json=payload, timeout=10)
            
            # Hata kontrolÃ¼
            if response.status_code != 200:
                return {
                    'success': False,
                    'error': f'Tavily API hatasÄ±: {response.status_code}',
                    'details': response.text
                }
            
            # SonuÃ§larÄ± parse et
            data = response.json()
            
            # SonuÃ§larÄ± formatla
            results = {
                'success': True,
                'query': query,
                'answer': data.get('answer', ''),  # AI tarafÄ±ndan oluÅŸturulan Ã¶zet
                'results': []
            }
            
            # Her bir arama sonucunu ekle
            for item in data.get('results', []):
                results['results'].append({
                    'title': item.get('title', ''),
                    'url': item.get('url', ''),
                    'content': item.get('content', ''),
                    'score': item.get('score', 0)
                })
            
            return results
            
        except requests.exceptions.Timeout:
            return {
                'success': False,
                'error': 'Arama zaman aÅŸÄ±mÄ±na uÄŸradÄ±. LÃ¼tfen tekrar deneyin.'
            }
        except requests.exceptions.RequestException as e:
            return {
                'success': False,
                'error': f'BaÄŸlantÄ± hatasÄ±: {str(e)}'
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'Beklenmeyen hata: {str(e)}'
            }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # NORMAL CHAT METODLARI
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_response(self, messages):
        try:
            if self.provider in ["groq", "openai", "deepinfra", "ollama", "cerebras"]:
                return self._get_openai_compatible_response(messages)
            
            elif self.provider == "anthropic":
                return self._get_anthropic_response(messages)
            
            elif self.provider == "google":
                return self._get_google_response(messages)
            
            elif self.provider == "cohere":
                return self._get_cohere_response(messages)
                
        except Exception as e:
            return f"âŒ Hata ({self.provider}): {str(e)}"
    
    def get_streaming_response(self, messages):
        try:
            if self.provider in ["groq", "openai", "deepinfra", "ollama", "cerebras"]:
                yield from self._get_openai_compatible_streaming(messages)
            
            elif self.provider == "anthropic":
                yield from self._get_anthropic_streaming(messages)
            
            elif self.provider == "google":
                yield from self._get_google_streaming(messages)
            
            elif self.provider == "cohere":
                yield from self._get_cohere_streaming(messages)
                
        except Exception as e:
            yield f"âŒ Hata ({self.provider}): {str(e)}"
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GROQ & OPENAI & DEEPINFRA & OLLAMA & CEREBRAS (OpenAI API Uyumlu)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _get_openai_compatible_response(self, messages):
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=Settings.TEMPERATURE,
            max_tokens=Settings.MAX_TOKENS,
            top_p=Settings.TOP_P,
            frequency_penalty=Settings.FREQUENCY_PENALTY,
            presence_penalty=Settings.PRESENCE_PENALTY,
        )
        return response.choices[0].message.content
    
    def _get_openai_compatible_streaming(self, messages):
        stream = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=Settings.TEMPERATURE,
            max_tokens=Settings.MAX_TOKENS,
            top_p=Settings.TOP_P,
            frequency_penalty=Settings.FREQUENCY_PENALTY,
            presence_penalty=Settings.PRESENCE_PENALTY,
            stream=True,
        )
        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ANTHROPIC (Claude)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _get_anthropic_response(self, messages):
        system_msg = ""
        user_messages = []
        
        for msg in messages:
            if msg["role"] == "system":
                system_msg = msg["content"]
            else:
                user_messages.append(msg)
        
        response = self.client.messages.create(
            model=self.model,
            max_tokens=Settings.MAX_TOKENS,
            temperature=Settings.TEMPERATURE,
            system=system_msg,
            messages=user_messages
        )
        return response.content[0].text
    
    def _get_anthropic_streaming(self, messages):
        system_msg = ""
        user_messages = []
        
        for msg in messages:
            if msg["role"] == "system":
                system_msg = msg["content"]
            else:
                user_messages.append(msg)
        
        with self.client.messages.stream(
            model=self.model,
            max_tokens=Settings.MAX_TOKENS,
            temperature=Settings.TEMPERATURE,
            system=system_msg,
            messages=user_messages
        ) as stream:
            for text in stream.text_stream:
                yield text
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GOOGLE (Gemini) - TAM ENTEGRASYONU YENÄ°DEN YAZILDI
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _get_google_response(self, messages):
        """Gemini API - Normal yanÄ±t"""
        try:
            # Model oluÅŸtur
            model = self.client.GenerativeModel(
                model_name=self.model,
                generation_config={
                    "temperature": Settings.TEMPERATURE,
                    "top_p": Settings.TOP_P,
                    "max_output_tokens": Settings.MAX_TOKENS,
                }
            )
            
            # MesajlarÄ± dÃ¶nÃ¼ÅŸtÃ¼r
            chat_history, current_prompt = self._convert_messages_for_gemini(messages)
            
            # Chat baÅŸlat
            if chat_history:
                chat = model.start_chat(history=chat_history)
                response = chat.send_message(current_prompt)
            else:
                # Ä°lk mesaj
                response = model.generate_content(current_prompt)
            
            # Response'dan text Ã§Ä±kar
            if hasattr(response, "text") and response.text:
                return response.text
            
            if hasattr(response, "candidates"):
                parts = response.candidates[0].content.parts
                return "".join(
                    part.text for part in parts if hasattr(part, "text")
                )
            
            return ""
            
        except Exception as e:
            return f"âŒ Gemini HatasÄ±: {str(e)}"
    
    def _get_google_streaming(self, messages):
        """Gemini API - Streaming yanÄ±t"""
        try:
            # Model oluÅŸtur
            model = self.client.GenerativeModel(
                model_name=self.model,
                generation_config={
                    "temperature": Settings.TEMPERATURE,
                    "top_p": Settings.TOP_P,
                    "max_output_tokens": Settings.MAX_TOKENS,
                }
            )
            
            # MesajlarÄ± dÃ¶nÃ¼ÅŸtÃ¼r
            chat_history, current_prompt = self._convert_messages_for_gemini(messages)
            
            # Chat baÅŸlat ve stream
            if chat_history:
                chat = model.start_chat(history=chat_history)
                response = chat.send_message(current_prompt, stream=True)
            else:
                # Ä°lk mesaj
                response = model.generate_content(current_prompt, stream=True)
            
            # Stream yanÄ±tÄ±
            for chunk in response:
                if chunk.text:
                    yield chunk.text
                    
        except Exception as e:
            yield f"âŒ Gemini HatasÄ±: {str(e)}"
    
    def _convert_messages_for_gemini(self, messages):
        """
        OpenAI formatÄ±ndaki mesajlarÄ± Gemini formatÄ±na Ã§evir
        
        OpenAI format:
        [
            {"role": "system", "content": "..."},
            {"role": "user", "content": "..."},
            {"role": "assistant", "content": "..."}
        ]
        
        Gemini format:
        history = [
            {"role": "user", "parts": ["..."]},
            {"role": "model", "parts": ["..."]}
        ]
        current_message = "son kullanÄ±cÄ± mesajÄ±"
        """
        
        chat_history = []
        system_instruction = ""
        current_prompt = ""
        
        for i, msg in enumerate(messages):
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            # System mesajÄ±nÄ± sistem talimatÄ± olarak ekle
            if role == "system":
                system_instruction = content
                continue
            
            # Son mesaj kullanÄ±cÄ±dan ise, onu current_prompt olarak sakla
            if i == len(messages) - 1 and role == "user":
                # EÄŸer system instruction varsa, baÅŸÄ±na ekle
                if system_instruction:
                    current_prompt = f"{system_instruction}\n\n{content}"
                else:
                    current_prompt = content
            else:
                # GeÃ§miÅŸ mesajlarÄ± ekle
                if role == "user":
                    chat_history.append({
                        "role": "user",
                        "parts": [content]
                    })
                elif role == "assistant":
                    chat_history.append({
                        "role": "model",
                        "parts": [content]
                    })
        
        # EÄŸer current_prompt boÅŸsa (Ã¶rneÄŸin sadece system mesajÄ± varsa)
        if not current_prompt and messages:
            last_msg = messages[-1]
            if last_msg.get("role") == "user":
                content = last_msg.get("content", "")
                if system_instruction:
                    current_prompt = f"{system_instruction}\n\n{content}"
                else:
                    current_prompt = content
        
        return chat_history, current_prompt
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # COHERE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _get_cohere_response(self, messages):
        prompt = messages[-1]["content"] if messages else ""
        
        chat_history = []
        for i, msg in enumerate(messages[:-1]):
            if msg["role"] == "system":
                continue
            chat_history.append({
                "role": "USER" if msg["role"] == "user" else "CHATBOT",
                "message": msg["content"]
            })
        
        response = self.client.chat(
            model=self.model,
            message=prompt,
            chat_history=chat_history,
            temperature=Settings.TEMPERATURE,
        )
        return response.text
    
    def _get_cohere_streaming(self, messages):
        prompt = messages[-1]["content"] if messages else ""
        
        chat_history = []
        for msg in messages[:-1]:
            if msg["role"] == "system":
                continue
            chat_history.append({
                "role": "USER" if msg["role"] == "user" else "CHATBOT",
                "message": msg["content"]
            })
        
        response = self.client.chat_stream(
            model=self.model,
            message=prompt,
            chat_history=chat_history,
            temperature=Settings.TEMPERATURE,
        )
        
        for event in response:
            if event.event_type == "text-generation":
                yield event.text


# Eski sÄ±nÄ±f adÄ± ile uyumluluk iÃ§in
GroqAPIHandler = MultiProviderAPIHandler